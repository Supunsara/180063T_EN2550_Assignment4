{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "K = len(np.unique(y_train)) # Classes\n",
    "Ntr = x_train.shape[0]\n",
    "Nte = x_test.shape[0]\n",
    "Din = 3072 # CIFAR10\n",
    "# Din = 784 # MINIST\n",
    "\n",
    "# Normalize pixel values\n",
    "#x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0)\n",
    "x_train = x_train - mean_image\n",
    "x_test = x_test - mean_image\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
    "\n",
    "x_train = np.reshape(x_train,(Ntr,Din))\n",
    "x_test = np.reshape(x_test,(Nte,Din))\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "std=1e-5\n",
    "H= 200\n",
    "w1 = std*np.random.randn(Din,H)\n",
    "w2 = std*np.random.randn(H,K)\n",
    "b1 = np.zeros(H)\n",
    "b2 = np.zeros(K)\n",
    "batch_size = 500\n",
    "\n",
    "iterations = 300\n",
    "lr = 1.4e-2\n",
    "lr_decay=0.999\n",
    "reg = 5e-6\n",
    "loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iteration 0 / 300: loss 0.797861\n",
      "iteration 1 / 300: loss 0.766751\n",
      "iteration 2 / 300: loss 0.762217\n",
      "iteration 3 / 300: loss 0.770548\n",
      "iteration 4 / 300: loss 0.745263\n",
      "iteration 5 / 300: loss 0.739556\n",
      "iteration 6 / 300: loss 0.713454\n",
      "iteration 7 / 300: loss 0.705209\n",
      "iteration 8 / 300: loss 0.709619\n",
      "iteration 9 / 300: loss 0.681471\n",
      "iteration 10 / 300: loss 0.691875\n",
      "iteration 11 / 300: loss 0.676296\n",
      "iteration 12 / 300: loss 0.675843\n",
      "iteration 13 / 300: loss 0.674922\n",
      "iteration 14 / 300: loss 0.641563\n",
      "iteration 15 / 300: loss 0.652052\n",
      "iteration 16 / 300: loss 0.669520\n",
      "iteration 17 / 300: loss 0.628202\n",
      "iteration 18 / 300: loss 0.635810\n",
      "iteration 19 / 300: loss 0.659641\n",
      "iteration 20 / 300: loss 0.614135\n",
      "iteration 21 / 300: loss 0.609187\n",
      "iteration 22 / 300: loss 0.604186\n",
      "iteration 23 / 300: loss 0.616057\n",
      "iteration 24 / 300: loss 0.653306\n",
      "iteration 25 / 300: loss 0.597200\n",
      "iteration 26 / 300: loss 0.604123\n",
      "iteration 27 / 300: loss 0.590445\n",
      "iteration 28 / 300: loss 0.591348\n",
      "iteration 29 / 300: loss 0.581034\n",
      "iteration 30 / 300: loss 0.597957\n",
      "iteration 31 / 300: loss 0.607036\n",
      "iteration 32 / 300: loss 0.602834\n",
      "iteration 33 / 300: loss 0.614287\n",
      "iteration 34 / 300: loss 0.635683\n",
      "iteration 35 / 300: loss 0.600081\n",
      "iteration 36 / 300: loss 0.589015\n",
      "iteration 37 / 300: loss 0.579737\n",
      "iteration 38 / 300: loss 0.601532\n",
      "iteration 39 / 300: loss 0.598461\n",
      "iteration 40 / 300: loss 0.608076\n",
      "iteration 41 / 300: loss 0.583564\n",
      "iteration 42 / 300: loss 0.595856\n",
      "iteration 43 / 300: loss 0.583801\n",
      "iteration 44 / 300: loss 0.601778\n",
      "iteration 45 / 300: loss 0.610466\n",
      "iteration 46 / 300: loss 0.602529\n",
      "iteration 47 / 300: loss 0.574012\n",
      "iteration 48 / 300: loss 0.590175\n",
      "iteration 49 / 300: loss 0.584282\n",
      "iteration 50 / 300: loss 0.606668\n",
      "iteration 51 / 300: loss 0.614508\n",
      "iteration 52 / 300: loss 0.574782\n",
      "iteration 53 / 300: loss 0.574389\n",
      "iteration 54 / 300: loss 0.589895\n",
      "iteration 55 / 300: loss 0.601467\n",
      "iteration 56 / 300: loss 0.602167\n",
      "iteration 57 / 300: loss 0.579936\n",
      "iteration 58 / 300: loss 0.575616\n",
      "iteration 59 / 300: loss 0.547745\n",
      "iteration 60 / 300: loss 0.600108\n",
      "iteration 61 / 300: loss 0.589498\n",
      "iteration 62 / 300: loss 0.598024\n",
      "iteration 63 / 300: loss 0.563957\n",
      "iteration 64 / 300: loss 0.595706\n",
      "iteration 65 / 300: loss 0.593158\n",
      "iteration 66 / 300: loss 0.606136\n",
      "iteration 67 / 300: loss 0.569629\n",
      "iteration 68 / 300: loss 0.587369\n",
      "iteration 69 / 300: loss 0.579840\n",
      "iteration 70 / 300: loss 0.596352\n",
      "iteration 71 / 300: loss 0.568716\n",
      "iteration 72 / 300: loss 0.582277\n",
      "iteration 73 / 300: loss 0.593186\n",
      "iteration 74 / 300: loss 0.575607\n",
      "iteration 75 / 300: loss 0.586202\n",
      "iteration 76 / 300: loss 0.584138\n",
      "iteration 77 / 300: loss 0.572108\n",
      "iteration 78 / 300: loss 0.595412\n",
      "iteration 79 / 300: loss 0.572296\n",
      "iteration 80 / 300: loss 0.584609\n",
      "iteration 81 / 300: loss 0.585924\n",
      "iteration 82 / 300: loss 0.597588\n",
      "iteration 83 / 300: loss 0.597266\n",
      "iteration 84 / 300: loss 0.617393\n",
      "iteration 85 / 300: loss 0.602276\n",
      "iteration 86 / 300: loss 0.584739\n",
      "iteration 87 / 300: loss 0.607220\n",
      "iteration 88 / 300: loss 0.591917\n",
      "iteration 89 / 300: loss 0.570290\n",
      "iteration 90 / 300: loss 0.594649\n",
      "iteration 91 / 300: loss 0.574442\n",
      "iteration 92 / 300: loss 0.592603\n",
      "iteration 93 / 300: loss 0.579993\n",
      "iteration 94 / 300: loss 0.610142\n",
      "iteration 95 / 300: loss 0.586606\n",
      "iteration 96 / 300: loss 0.556937\n",
      "iteration 97 / 300: loss 0.569226\n",
      "iteration 98 / 300: loss 0.576544\n",
      "iteration 99 / 300: loss 0.571465\n",
      "iteration 100 / 300: loss 0.579680\n",
      "iteration 101 / 300: loss 0.564159\n",
      "iteration 102 / 300: loss 0.626636\n",
      "iteration 103 / 300: loss 0.570696\n",
      "iteration 104 / 300: loss 0.584277\n",
      "iteration 105 / 300: loss 0.581408\n",
      "iteration 106 / 300: loss 0.577298\n",
      "iteration 107 / 300: loss 0.561064\n",
      "iteration 108 / 300: loss 0.585325\n",
      "iteration 109 / 300: loss 0.598274\n",
      "iteration 110 / 300: loss 0.593974\n",
      "iteration 111 / 300: loss 0.593093\n",
      "iteration 112 / 300: loss 0.604150\n",
      "iteration 113 / 300: loss 0.587918\n",
      "iteration 114 / 300: loss 0.564142\n",
      "iteration 115 / 300: loss 0.584039\n",
      "iteration 116 / 300: loss 0.580844\n",
      "iteration 117 / 300: loss 0.597339\n",
      "iteration 118 / 300: loss 0.590081\n",
      "iteration 119 / 300: loss 0.560398\n",
      "iteration 120 / 300: loss 0.593763\n",
      "iteration 121 / 300: loss 0.590111\n",
      "iteration 122 / 300: loss 0.584637\n",
      "iteration 123 / 300: loss 0.570836\n",
      "iteration 124 / 300: loss 0.567375\n",
      "iteration 125 / 300: loss 0.558178\n",
      "iteration 126 / 300: loss 0.577934\n",
      "iteration 127 / 300: loss 0.566986\n",
      "iteration 128 / 300: loss 0.608829\n",
      "iteration 129 / 300: loss 0.575654\n",
      "iteration 130 / 300: loss 0.589166\n",
      "iteration 131 / 300: loss 0.580016\n",
      "iteration 132 / 300: loss 0.569512\n",
      "iteration 133 / 300: loss 0.580024\n",
      "iteration 134 / 300: loss 0.611294\n",
      "iteration 135 / 300: loss 0.586686\n",
      "iteration 136 / 300: loss 0.580408\n",
      "iteration 137 / 300: loss 0.602561\n",
      "iteration 138 / 300: loss 0.589743\n",
      "iteration 139 / 300: loss 0.592425\n",
      "iteration 140 / 300: loss 0.612822\n",
      "iteration 141 / 300: loss 0.566158\n",
      "iteration 142 / 300: loss 0.573661\n",
      "iteration 143 / 300: loss 0.599003\n",
      "iteration 144 / 300: loss 0.586816\n",
      "iteration 145 / 300: loss 0.599242\n",
      "iteration 146 / 300: loss 0.595603\n",
      "iteration 147 / 300: loss 0.586699\n",
      "iteration 148 / 300: loss 0.592372\n",
      "iteration 149 / 300: loss 0.585171\n",
      "iteration 150 / 300: loss 0.582226\n",
      "iteration 151 / 300: loss 0.590690\n",
      "iteration 152 / 300: loss 0.583895\n",
      "iteration 153 / 300: loss 0.569982\n",
      "iteration 154 / 300: loss 0.600491\n",
      "iteration 155 / 300: loss 0.591891\n",
      "iteration 156 / 300: loss 0.586923\n",
      "iteration 157 / 300: loss 0.592394\n",
      "iteration 158 / 300: loss 0.603757\n",
      "iteration 159 / 300: loss 0.583929\n",
      "iteration 160 / 300: loss 0.601678\n",
      "iteration 161 / 300: loss 0.587680\n",
      "iteration 162 / 300: loss 0.595712\n",
      "iteration 163 / 300: loss 0.591110\n",
      "iteration 164 / 300: loss 0.567999\n",
      "iteration 165 / 300: loss 0.601633\n",
      "iteration 166 / 300: loss 0.593273\n",
      "iteration 167 / 300: loss 0.589619\n",
      "iteration 168 / 300: loss 0.578083\n",
      "iteration 169 / 300: loss 0.570098\n",
      "iteration 170 / 300: loss 0.590009\n",
      "iteration 171 / 300: loss 0.595812\n",
      "iteration 172 / 300: loss 0.596102\n",
      "iteration 173 / 300: loss 0.602202\n",
      "iteration 174 / 300: loss 0.578256\n",
      "iteration 175 / 300: loss 0.601480\n",
      "iteration 176 / 300: loss 0.575007\n",
      "iteration 177 / 300: loss 0.585374\n",
      "iteration 178 / 300: loss 0.580426\n",
      "iteration 179 / 300: loss 0.603331\n",
      "iteration 180 / 300: loss 0.576672\n",
      "iteration 181 / 300: loss 0.595560\n",
      "iteration 182 / 300: loss 0.592706\n",
      "iteration 183 / 300: loss 0.599258\n",
      "iteration 184 / 300: loss 0.569622\n",
      "iteration 185 / 300: loss 0.572380\n",
      "iteration 186 / 300: loss 0.595025\n",
      "iteration 187 / 300: loss 0.590089\n",
      "iteration 188 / 300: loss 0.599186\n",
      "iteration 189 / 300: loss 0.593380\n",
      "iteration 190 / 300: loss 0.595372\n",
      "iteration 191 / 300: loss 0.579221\n",
      "iteration 192 / 300: loss 0.574161\n",
      "iteration 193 / 300: loss 0.606371\n",
      "iteration 194 / 300: loss 0.573090\n",
      "iteration 195 / 300: loss 0.580679\n",
      "iteration 196 / 300: loss 0.583877\n",
      "iteration 197 / 300: loss 0.583540\n",
      "iteration 198 / 300: loss 0.610532\n",
      "iteration 199 / 300: loss 0.586984\n",
      "iteration 200 / 300: loss 0.602389\n",
      "iteration 201 / 300: loss 0.569754\n",
      "iteration 202 / 300: loss 0.581792\n",
      "iteration 203 / 300: loss 0.577371\n",
      "iteration 204 / 300: loss 0.584972\n",
      "iteration 205 / 300: loss 0.580492\n",
      "iteration 206 / 300: loss 0.593816\n",
      "iteration 207 / 300: loss 0.575271\n",
      "iteration 208 / 300: loss 0.592075\n",
      "iteration 209 / 300: loss 0.594147\n",
      "iteration 210 / 300: loss 0.614120\n",
      "iteration 211 / 300: loss 0.590607\n",
      "iteration 212 / 300: loss 0.586964\n",
      "iteration 213 / 300: loss 0.594585\n",
      "iteration 214 / 300: loss 0.581309\n",
      "iteration 215 / 300: loss 0.590042\n",
      "iteration 216 / 300: loss 0.585615\n",
      "iteration 217 / 300: loss 0.581935\n",
      "iteration 218 / 300: loss 0.586176\n",
      "iteration 219 / 300: loss 0.581090\n",
      "iteration 220 / 300: loss 0.592882\n",
      "iteration 221 / 300: loss 0.591743\n",
      "iteration 222 / 300: loss 0.577564\n",
      "iteration 223 / 300: loss 0.589098\n",
      "iteration 224 / 300: loss 0.603287\n",
      "iteration 225 / 300: loss 0.557928\n",
      "iteration 226 / 300: loss 0.589786\n",
      "iteration 227 / 300: loss 0.556503\n",
      "iteration 228 / 300: loss 0.581346\n",
      "iteration 229 / 300: loss 0.579022\n",
      "iteration 230 / 300: loss 0.578248\n",
      "iteration 231 / 300: loss 0.559409\n",
      "iteration 232 / 300: loss 0.586483\n",
      "iteration 233 / 300: loss 0.586823\n",
      "iteration 234 / 300: loss 0.589075\n",
      "iteration 235 / 300: loss 0.598465\n",
      "iteration 236 / 300: loss 0.599147\n",
      "iteration 237 / 300: loss 0.571749\n",
      "iteration 238 / 300: loss 0.575193\n",
      "iteration 239 / 300: loss 0.598150\n",
      "iteration 240 / 300: loss 0.571291\n",
      "iteration 241 / 300: loss 0.598444\n",
      "iteration 242 / 300: loss 0.576589\n",
      "iteration 243 / 300: loss 0.597567\n",
      "iteration 244 / 300: loss 0.570480\n",
      "iteration 245 / 300: loss 0.575959\n",
      "iteration 246 / 300: loss 0.567150\n",
      "iteration 247 / 300: loss 0.606960\n",
      "iteration 248 / 300: loss 0.586535\n",
      "iteration 249 / 300: loss 0.582747\n",
      "iteration 250 / 300: loss 0.596524\n",
      "iteration 251 / 300: loss 0.590930\n",
      "iteration 252 / 300: loss 0.591135\n",
      "iteration 253 / 300: loss 0.580776\n",
      "iteration 254 / 300: loss 0.569413\n",
      "iteration 255 / 300: loss 0.591392\n",
      "iteration 256 / 300: loss 0.595874\n",
      "iteration 257 / 300: loss 0.600875\n",
      "iteration 258 / 300: loss 0.587271\n",
      "iteration 259 / 300: loss 0.575515\n",
      "iteration 260 / 300: loss 0.595482\n",
      "iteration 261 / 300: loss 0.589298\n",
      "iteration 262 / 300: loss 0.572469\n",
      "iteration 263 / 300: loss 0.589389\n",
      "iteration 264 / 300: loss 0.588433\n",
      "iteration 265 / 300: loss 0.595228\n",
      "iteration 266 / 300: loss 0.581334\n",
      "iteration 267 / 300: loss 0.584057\n",
      "iteration 268 / 300: loss 0.601635\n",
      "iteration 269 / 300: loss 0.569491\n",
      "iteration 270 / 300: loss 0.593346\n",
      "iteration 271 / 300: loss 0.578222\n",
      "iteration 272 / 300: loss 0.596837\n",
      "iteration 273 / 300: loss 0.604294\n",
      "iteration 274 / 300: loss 0.569137\n",
      "iteration 275 / 300: loss 0.570802\n",
      "iteration 276 / 300: loss 0.580556\n",
      "iteration 277 / 300: loss 0.597326\n",
      "iteration 278 / 300: loss 0.621043\n",
      "iteration 279 / 300: loss 0.587472\n",
      "iteration 280 / 300: loss 0.581256\n",
      "iteration 281 / 300: loss 0.571907\n",
      "iteration 282 / 300: loss 0.579054\n",
      "iteration 283 / 300: loss 0.585358\n",
      "iteration 284 / 300: loss 0.584391\n",
      "iteration 285 / 300: loss 0.584260\n",
      "iteration 286 / 300: loss 0.580611\n",
      "iteration 287 / 300: loss 0.551988\n",
      "iteration 288 / 300: loss 0.579910\n",
      "iteration 289 / 300: loss 0.588781\n",
      "iteration 290 / 300: loss 0.587112\n",
      "iteration 291 / 300: loss 0.599957\n",
      "iteration 292 / 300: loss 0.584841\n",
      "iteration 293 / 300: loss 0.600494\n",
      "iteration 294 / 300: loss 0.583899\n",
      "iteration 295 / 300: loss 0.594944\n",
      "iteration 296 / 300: loss 0.591866\n",
      "iteration 297 / 300: loss 0.603893\n",
      "iteration 298 / 300: loss 0.564837\n",
      "iteration 299 / 300: loss 0.588236\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in range(iterations):\n",
    "    indices = np.arange(Ntr)\n",
    "    rng.shuffle(indices)\n",
    "\n",
    "    x = x_train[indices]\n",
    "    y = y_train[indices]\n",
    "\n",
    "    n = int(Ntr/batch_size)\n",
    "    x_batches = np.array_split(x,n)\n",
    "    y_batches = np.array_split(y,n)\n",
    "\n",
    "    for i in range(n):\n",
    "        h = 1.0/(1.0+np.exp(-(x_batches[i].dot(w1)+b1)))\n",
    "        y_pred = h.dot(w2)+b2\n",
    "        loss = 1./batch_size*np.square(y_pred-y_batches[i]).sum()+reg*(np.sum(w2*w2)+np.sum(w1*w1))\n",
    "        loss_history.append(loss)\n",
    "        dy_pred = 1./batch_size*2.0*(y_pred-y_batches[i]) #partial derivative of L w.r.t. y_hat\n",
    "        dw2 = h.T.dot(dy_pred)+reg*w2\n",
    "        db2 = dy_pred.sum(axis=0)\n",
    "        dh = dy_pred.dot(w2.T)\n",
    "        dw1 = x_batches[i].T.dot(dh*h*(1-h))+reg*w1\n",
    "        db1 = (dh*h*(1-h)).sum(axis=0)\n",
    "        w1 -= lr*dw1\n",
    "        w2 -= lr*dw2\n",
    "        b1 -= lr*db1\n",
    "        b2 -= lr*db2\n",
    "        lr *= lr_decay\n",
    "    print('iteration %d / %d: loss %f' %(t,iterations,loss))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_acc =  0.9985933333333333\ntrain_loss =  0.5843544134257277\ntest_acc =  0.9981444444444444\ntest_loss =  0.13853067579905712\n"
     ]
    }
   ],
   "source": [
    "indices1 = np.arange(Ntr)\n",
    "rng.shuffle(indices1)\n",
    "\n",
    "indices2 = np.arange(Nte)\n",
    "rng.shuffle(indices2)\n",
    "\n",
    "x_tr = x_train[indices1]\n",
    "y_tr = y_train[indices1]\n",
    "\n",
    "x_te = x_test[indices2]\n",
    "y_te = y_test[indices2]\n",
    "\n",
    "n = int(Ntr/batch_size)\n",
    "x_batches1 = np.array_split(x_tr,n)\n",
    "y_batches1 = np.array_split(y_tr,n)\n",
    "\n",
    "x_batches2 = np.array_split(x_te,n)\n",
    "y_batches2 = np.array_split(y_te,n)\n",
    "\n",
    "\n",
    "for i in range(n):\n",
    "    #train accuracy,train loss\n",
    "    h = 1.0/(1.0+np.exp(-(x_batches1[i].dot(w1)+b1)))\n",
    "    y_pred = h.dot(w2)+b2\n",
    "    train_acc = 1.0 -1/(9*Ntr)*(np.abs(np.argmax(y_batches1[i],axis=1) - np.argmax(y_pred, axis=1))).sum()\n",
    "    #train_acc = 1/Ntr*(np.sum(np.argmax(y_pred,axis=1)==np.argmax(y_batches1[i],axis=1)))\n",
    "    train_loss = 1./batch_size*np.square(y_pred-y_batches1[i]).sum()+reg*(np.sum(w2*w2)+np.sum(w1*w1))\n",
    "\n",
    "    #test accuracy, test loss\n",
    "    h = 1.0/(1.0+np.exp(-(x_batches2[i].dot(w1)+b1)))\n",
    "    y_pred = h.dot(w2)+b2\n",
    "    test_acc = 1.0 - 1/(9*Nte)*(np.abs(np.argmax(y_batches2[i],axis=1) - np.argmax(y_pred, axis=1))).sum()\n",
    "    #test_acc = 1/Nte*(np.sum(np.argmax(y_pred,axis=1)==np.argmax(y_batches2[i],axis=1)))\n",
    "    test_loss = 1./batch_size*np.square(y_pred-y_batches2[i]).sum()+reg*(np.sum(w2*w2)+np.sum(w1*w1))\n",
    "\n",
    "print(\"train_acc = \", train_acc)\n",
    "print(\"train_loss = \", train_loss)\n",
    "print(\"test_acc = \", test_acc)\n",
    "print(\"test_loss = \", test_loss)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}