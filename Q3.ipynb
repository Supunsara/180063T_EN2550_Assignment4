{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "K = len(np.unique(y_train)) # Classes\n",
    "Ntr = x_train.shape[0]\n",
    "Nte = x_test.shape[0]\n",
    "Din = 3072 # CIFAR10\n",
    "# Din = 784 # MINIST\n",
    "\n",
    "# Normalize pixel values\n",
    "#x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "mean_image = np.mean(x_train, axis=0)\n",
    "x_train = x_train - mean_image\n",
    "x_test = x_test - mean_image\n",
    "\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\n",
    "\n",
    "x_train = np.reshape(x_train,(Ntr,Din))\n",
    "x_test = np.reshape(x_test,(Nte,Din))\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "\n",
    "std=1e-5\n",
    "H= 200\n",
    "w1 = std*np.random.randn(Din,H)\n",
    "w2 = std*np.random.randn(H,K)\n",
    "b1 = np.zeros(H)\n",
    "b2 = np.zeros(K)\n",
    "batch_size = 500\n",
    "\n",
    "iterations = 300\n",
    "lr = 1.4e-2\n",
    "lr_decay=0.999\n",
    "reg = 5e-6\n",
    "loss_history = []\n",
    "train_acc_history = []\n",
    "val_acc_history = []\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "iteration 0 / 300: loss 0.793045\n",
      "iteration 1 / 300: loss 0.769099\n",
      "iteration 2 / 300: loss 0.761735\n",
      "iteration 3 / 300: loss 0.769830\n",
      "iteration 4 / 300: loss 0.747253\n",
      "iteration 5 / 300: loss 0.740631\n",
      "iteration 6 / 300: loss 0.712433\n",
      "iteration 7 / 300: loss 0.703670\n",
      "iteration 8 / 300: loss 0.704660\n",
      "iteration 9 / 300: loss 0.680686\n",
      "iteration 10 / 300: loss 0.690268\n",
      "iteration 11 / 300: loss 0.674384\n",
      "iteration 12 / 300: loss 0.674291\n",
      "iteration 13 / 300: loss 0.681925\n",
      "iteration 14 / 300: loss 0.640128\n",
      "iteration 15 / 300: loss 0.648779\n",
      "iteration 16 / 300: loss 0.659229\n",
      "iteration 17 / 300: loss 0.638731\n",
      "iteration 18 / 300: loss 0.632524\n",
      "iteration 19 / 300: loss 0.665128\n",
      "iteration 20 / 300: loss 0.618925\n",
      "iteration 21 / 300: loss 0.612948\n",
      "iteration 22 / 300: loss 0.608820\n",
      "iteration 23 / 300: loss 0.621480\n",
      "iteration 24 / 300: loss 0.642806\n",
      "iteration 25 / 300: loss 0.602547\n",
      "iteration 26 / 300: loss 0.610290\n",
      "iteration 27 / 300: loss 0.589837\n",
      "iteration 28 / 300: loss 0.590453\n",
      "iteration 29 / 300: loss 0.586326\n",
      "iteration 30 / 300: loss 0.593272\n",
      "iteration 31 / 300: loss 0.613616\n",
      "iteration 32 / 300: loss 0.601846\n",
      "iteration 33 / 300: loss 0.616414\n",
      "iteration 34 / 300: loss 0.628101\n",
      "iteration 35 / 300: loss 0.602535\n",
      "iteration 36 / 300: loss 0.592693\n",
      "iteration 37 / 300: loss 0.578289\n",
      "iteration 38 / 300: loss 0.600291\n",
      "iteration 39 / 300: loss 0.593770\n",
      "iteration 40 / 300: loss 0.611500\n",
      "iteration 41 / 300: loss 0.589487\n",
      "iteration 42 / 300: loss 0.595729\n",
      "iteration 43 / 300: loss 0.588343\n",
      "iteration 44 / 300: loss 0.593054\n",
      "iteration 45 / 300: loss 0.612754\n",
      "iteration 46 / 300: loss 0.605210\n",
      "iteration 47 / 300: loss 0.573129\n",
      "iteration 48 / 300: loss 0.586359\n",
      "iteration 49 / 300: loss 0.590919\n",
      "iteration 50 / 300: loss 0.597179\n",
      "iteration 51 / 300: loss 0.608930\n",
      "iteration 52 / 300: loss 0.570745\n",
      "iteration 53 / 300: loss 0.573742\n",
      "iteration 54 / 300: loss 0.593929\n",
      "iteration 55 / 300: loss 0.602927\n",
      "iteration 56 / 300: loss 0.605162\n",
      "iteration 57 / 300: loss 0.582069\n",
      "iteration 58 / 300: loss 0.571074\n",
      "iteration 59 / 300: loss 0.554515\n",
      "iteration 60 / 300: loss 0.608520\n",
      "iteration 61 / 300: loss 0.587932\n",
      "iteration 62 / 300: loss 0.602971\n",
      "iteration 63 / 300: loss 0.562060\n",
      "iteration 64 / 300: loss 0.603210\n",
      "iteration 65 / 300: loss 0.589274\n",
      "iteration 66 / 300: loss 0.604849\n",
      "iteration 67 / 300: loss 0.574236\n",
      "iteration 68 / 300: loss 0.586341\n",
      "iteration 69 / 300: loss 0.592167\n",
      "iteration 70 / 300: loss 0.591070\n",
      "iteration 71 / 300: loss 0.575680\n",
      "iteration 72 / 300: loss 0.584350\n",
      "iteration 73 / 300: loss 0.593171\n",
      "iteration 74 / 300: loss 0.581808\n",
      "iteration 75 / 300: loss 0.590986\n",
      "iteration 76 / 300: loss 0.583228\n",
      "iteration 77 / 300: loss 0.571497\n",
      "iteration 78 / 300: loss 0.601003\n",
      "iteration 79 / 300: loss 0.574914\n",
      "iteration 80 / 300: loss 0.590484\n",
      "iteration 81 / 300: loss 0.593523\n",
      "iteration 82 / 300: loss 0.595530\n",
      "iteration 83 / 300: loss 0.601226\n",
      "iteration 84 / 300: loss 0.612988\n",
      "iteration 85 / 300: loss 0.598253\n",
      "iteration 86 / 300: loss 0.581987\n",
      "iteration 87 / 300: loss 0.599165\n",
      "iteration 88 / 300: loss 0.587236\n",
      "iteration 89 / 300: loss 0.577876\n",
      "iteration 90 / 300: loss 0.607197\n",
      "iteration 91 / 300: loss 0.575885\n",
      "iteration 92 / 300: loss 0.597589\n",
      "iteration 93 / 300: loss 0.574798\n",
      "iteration 94 / 300: loss 0.610095\n",
      "iteration 95 / 300: loss 0.582732\n",
      "iteration 96 / 300: loss 0.554051\n",
      "iteration 97 / 300: loss 0.581597\n",
      "iteration 98 / 300: loss 0.583041\n",
      "iteration 99 / 300: loss 0.577244\n",
      "iteration 100 / 300: loss 0.582115\n",
      "iteration 101 / 300: loss 0.575223\n",
      "iteration 102 / 300: loss 0.631993\n",
      "iteration 103 / 300: loss 0.565763\n",
      "iteration 104 / 300: loss 0.592170\n",
      "iteration 105 / 300: loss 0.579634\n",
      "iteration 106 / 300: loss 0.575575\n",
      "iteration 107 / 300: loss 0.564920\n",
      "iteration 108 / 300: loss 0.589177\n",
      "iteration 109 / 300: loss 0.607249\n",
      "iteration 110 / 300: loss 0.594886\n",
      "iteration 111 / 300: loss 0.590117\n",
      "iteration 112 / 300: loss 0.609742\n",
      "iteration 113 / 300: loss 0.583202\n",
      "iteration 114 / 300: loss 0.568688\n",
      "iteration 115 / 300: loss 0.583354\n",
      "iteration 116 / 300: loss 0.586750\n",
      "iteration 117 / 300: loss 0.595905\n",
      "iteration 118 / 300: loss 0.602256\n",
      "iteration 119 / 300: loss 0.570904\n",
      "iteration 120 / 300: loss 0.595834\n",
      "iteration 121 / 300: loss 0.589991\n",
      "iteration 122 / 300: loss 0.595138\n",
      "iteration 123 / 300: loss 0.568861\n",
      "iteration 124 / 300: loss 0.569680\n",
      "iteration 125 / 300: loss 0.564260\n",
      "iteration 126 / 300: loss 0.577646\n",
      "iteration 127 / 300: loss 0.572864\n",
      "iteration 128 / 300: loss 0.601905\n",
      "iteration 129 / 300: loss 0.569841\n",
      "iteration 130 / 300: loss 0.589784\n",
      "iteration 131 / 300: loss 0.587769\n",
      "iteration 132 / 300: loss 0.570473\n",
      "iteration 133 / 300: loss 0.572444\n",
      "iteration 134 / 300: loss 0.617842\n",
      "iteration 135 / 300: loss 0.586387\n",
      "iteration 136 / 300: loss 0.592884\n",
      "iteration 137 / 300: loss 0.595359\n",
      "iteration 138 / 300: loss 0.597493\n",
      "iteration 139 / 300: loss 0.596772\n",
      "iteration 140 / 300: loss 0.611588\n",
      "iteration 141 / 300: loss 0.572238\n",
      "iteration 142 / 300: loss 0.577758\n",
      "iteration 143 / 300: loss 0.601558\n",
      "iteration 144 / 300: loss 0.595095\n",
      "iteration 145 / 300: loss 0.596261\n",
      "iteration 146 / 300: loss 0.592146\n",
      "iteration 147 / 300: loss 0.586060\n",
      "iteration 148 / 300: loss 0.597216\n",
      "iteration 149 / 300: loss 0.578798\n",
      "iteration 150 / 300: loss 0.587942\n",
      "iteration 151 / 300: loss 0.584787\n",
      "iteration 152 / 300: loss 0.587321\n",
      "iteration 153 / 300: loss 0.570286\n",
      "iteration 154 / 300: loss 0.603408\n",
      "iteration 155 / 300: loss 0.594432\n",
      "iteration 156 / 300: loss 0.588082\n",
      "iteration 157 / 300: loss 0.601716\n",
      "iteration 158 / 300: loss 0.606129\n",
      "iteration 159 / 300: loss 0.578762\n",
      "iteration 160 / 300: loss 0.592234\n",
      "iteration 161 / 300: loss 0.584555\n",
      "iteration 162 / 300: loss 0.592529\n",
      "iteration 163 / 300: loss 0.589236\n",
      "iteration 164 / 300: loss 0.568207\n",
      "iteration 165 / 300: loss 0.612370\n",
      "iteration 166 / 300: loss 0.591709\n",
      "iteration 167 / 300: loss 0.593454\n",
      "iteration 168 / 300: loss 0.580568\n",
      "iteration 169 / 300: loss 0.575373\n",
      "iteration 170 / 300: loss 0.593268\n",
      "iteration 171 / 300: loss 0.599802\n",
      "iteration 172 / 300: loss 0.588874\n",
      "iteration 173 / 300: loss 0.597580\n",
      "iteration 174 / 300: loss 0.575686\n",
      "iteration 175 / 300: loss 0.599322\n",
      "iteration 176 / 300: loss 0.588450\n",
      "iteration 177 / 300: loss 0.592334\n",
      "iteration 178 / 300: loss 0.579118\n",
      "iteration 179 / 300: loss 0.605381\n",
      "iteration 180 / 300: loss 0.579474\n",
      "iteration 181 / 300: loss 0.593502\n",
      "iteration 182 / 300: loss 0.589410\n",
      "iteration 183 / 300: loss 0.602445\n",
      "iteration 184 / 300: loss 0.571043\n",
      "iteration 185 / 300: loss 0.569129\n",
      "iteration 186 / 300: loss 0.596758\n",
      "iteration 187 / 300: loss 0.594104\n",
      "iteration 188 / 300: loss 0.596778\n",
      "iteration 189 / 300: loss 0.589352\n",
      "iteration 190 / 300: loss 0.589450\n",
      "iteration 191 / 300: loss 0.589746\n",
      "iteration 192 / 300: loss 0.577613\n",
      "iteration 193 / 300: loss 0.609996\n",
      "iteration 194 / 300: loss 0.571354\n",
      "iteration 195 / 300: loss 0.587187\n",
      "iteration 196 / 300: loss 0.579915\n",
      "iteration 197 / 300: loss 0.578791\n",
      "iteration 198 / 300: loss 0.607902\n",
      "iteration 199 / 300: loss 0.589631\n",
      "iteration 200 / 300: loss 0.604558\n",
      "iteration 201 / 300: loss 0.566082\n",
      "iteration 202 / 300: loss 0.583550\n",
      "iteration 203 / 300: loss 0.589628\n",
      "iteration 204 / 300: loss 0.584630\n",
      "iteration 205 / 300: loss 0.582245\n",
      "iteration 206 / 300: loss 0.592031\n",
      "iteration 207 / 300: loss 0.578728\n",
      "iteration 208 / 300: loss 0.594984\n",
      "iteration 209 / 300: loss 0.600639\n",
      "iteration 210 / 300: loss 0.617909\n",
      "iteration 211 / 300: loss 0.596297\n",
      "iteration 212 / 300: loss 0.585210\n",
      "iteration 213 / 300: loss 0.585374\n",
      "iteration 214 / 300: loss 0.576166\n",
      "iteration 215 / 300: loss 0.600140\n",
      "iteration 216 / 300: loss 0.588234\n",
      "iteration 217 / 300: loss 0.589338\n",
      "iteration 218 / 300: loss 0.583018\n",
      "iteration 219 / 300: loss 0.576629\n",
      "iteration 220 / 300: loss 0.587334\n",
      "iteration 221 / 300: loss 0.581956\n",
      "iteration 222 / 300: loss 0.575889\n",
      "iteration 223 / 300: loss 0.585410\n",
      "iteration 224 / 300: loss 0.598164\n",
      "iteration 225 / 300: loss 0.562205\n",
      "iteration 226 / 300: loss 0.578628\n",
      "iteration 227 / 300: loss 0.560518\n",
      "iteration 228 / 300: loss 0.584486\n",
      "iteration 229 / 300: loss 0.583457\n",
      "iteration 230 / 300: loss 0.582906\n",
      "iteration 231 / 300: loss 0.552677\n",
      "iteration 232 / 300: loss 0.589289\n",
      "iteration 233 / 300: loss 0.590287\n",
      "iteration 234 / 300: loss 0.584506\n",
      "iteration 235 / 300: loss 0.592232\n",
      "iteration 236 / 300: loss 0.599991\n",
      "iteration 237 / 300: loss 0.579600\n",
      "iteration 238 / 300: loss 0.578477\n",
      "iteration 239 / 300: loss 0.592546\n",
      "iteration 240 / 300: loss 0.568321\n",
      "iteration 241 / 300: loss 0.600842\n",
      "iteration 242 / 300: loss 0.580453\n",
      "iteration 243 / 300: loss 0.606104\n",
      "iteration 244 / 300: loss 0.577777\n",
      "iteration 245 / 300: loss 0.574492\n",
      "iteration 246 / 300: loss 0.570287\n",
      "iteration 247 / 300: loss 0.613245\n",
      "iteration 248 / 300: loss 0.585885\n",
      "iteration 249 / 300: loss 0.580050\n",
      "iteration 250 / 300: loss 0.595364\n",
      "iteration 251 / 300: loss 0.601650\n",
      "iteration 252 / 300: loss 0.591944\n",
      "iteration 253 / 300: loss 0.586656\n",
      "iteration 254 / 300: loss 0.577351\n",
      "iteration 255 / 300: loss 0.589144\n",
      "iteration 256 / 300: loss 0.600136\n",
      "iteration 257 / 300: loss 0.596406\n",
      "iteration 258 / 300: loss 0.585358\n",
      "iteration 259 / 300: loss 0.573547\n",
      "iteration 260 / 300: loss 0.603965\n",
      "iteration 261 / 300: loss 0.584839\n",
      "iteration 262 / 300: loss 0.580112\n",
      "iteration 263 / 300: loss 0.596074\n",
      "iteration 264 / 300: loss 0.587596\n",
      "iteration 265 / 300: loss 0.589745\n",
      "iteration 266 / 300: loss 0.586514\n",
      "iteration 267 / 300: loss 0.584289\n",
      "iteration 268 / 300: loss 0.600224\n",
      "iteration 269 / 300: loss 0.580718\n",
      "iteration 270 / 300: loss 0.587898\n",
      "iteration 271 / 300: loss 0.582426\n",
      "iteration 272 / 300: loss 0.590182\n",
      "iteration 273 / 300: loss 0.594465\n",
      "iteration 274 / 300: loss 0.568548\n",
      "iteration 275 / 300: loss 0.583190\n",
      "iteration 276 / 300: loss 0.582881\n",
      "iteration 277 / 300: loss 0.600233\n",
      "iteration 278 / 300: loss 0.613274\n",
      "iteration 279 / 300: loss 0.579619\n",
      "iteration 280 / 300: loss 0.576890\n",
      "iteration 281 / 300: loss 0.580349\n",
      "iteration 282 / 300: loss 0.582903\n",
      "iteration 283 / 300: loss 0.581761\n",
      "iteration 284 / 300: loss 0.588405\n",
      "iteration 285 / 300: loss 0.581463\n",
      "iteration 286 / 300: loss 0.586637\n",
      "iteration 287 / 300: loss 0.568315\n",
      "iteration 288 / 300: loss 0.583443\n",
      "iteration 289 / 300: loss 0.589666\n",
      "iteration 290 / 300: loss 0.593242\n",
      "iteration 291 / 300: loss 0.596901\n",
      "iteration 292 / 300: loss 0.589064\n",
      "iteration 293 / 300: loss 0.604718\n",
      "iteration 294 / 300: loss 0.582763\n",
      "iteration 295 / 300: loss 0.593030\n",
      "iteration 296 / 300: loss 0.590381\n",
      "iteration 297 / 300: loss 0.588801\n",
      "iteration 298 / 300: loss 0.576243\n",
      "iteration 299 / 300: loss 0.585786\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for t in range(iterations):\n",
    "    indices = np.arange(Ntr)\n",
    "    rng.shuffle(indices)\n",
    "    #batch_indices = np.random.choice(Ntr,batch_size)\n",
    "    x = x_train[indices]\n",
    "    y = y_train[indices]\n",
    "\n",
    "    n = int(Ntr/batch_size)\n",
    "    x_batches = np.array_split(x,n)\n",
    "    y_batches = np.array_split(y,n)\n",
    "\n",
    "    for i in range(n):\n",
    "        h = 1.0/(1.0+np.exp(-(x_batches[i].dot(w1)+b1)))\n",
    "        y_pred = h.dot(w2)+b2\n",
    "        loss = 1./batch_size*np.square(y_pred-y_batches[i]).sum()+reg*(np.sum(w2*w2)+np.sum(w1*w1))\n",
    "        loss_history.append(loss)\n",
    "        dy_pred = 1./batch_size*2.0*(y_pred-y_batches[i]) #partial derivative of L w.r.t. y_hat\n",
    "        dw2 = h.T.dot(dy_pred)+reg*w2\n",
    "        db2 = dy_pred.sum(axis=0)\n",
    "        dh = dy_pred.dot(w2.T)\n",
    "        dw1 = x_batches[i].T.dot(dh*h*(1-h))+reg*w1\n",
    "        db1 = (dh*h*(1-h)).sum(axis=0)\n",
    "        w1 -= lr*dw1\n",
    "        w2 -= lr*dw2\n",
    "        b1 -= lr*db1\n",
    "        b2 -= lr*db2\n",
    "        lr *= lr_decay\n",
    "    print('iteration %d / %d: loss %f' %(t,iterations,loss))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train_acc =  0.868\ntrain_loss =  0.5888579116872218\ntest_acc =  0.7775555555555556\ntest_loss =  0.7327051792320263\n"
     ]
    }
   ],
   "source": [
    "indices1 = np.arange(Ntr)\n",
    "rng.shuffle(indices1)\n",
    "\n",
    "indices2 = np.arange(Nte)\n",
    "rng.shuffle(indices2)\n",
    "\n",
    "x_tr = x_train[indices1]\n",
    "y_tr = y_train[indices1]\n",
    "\n",
    "x_te = x_test[indices2]\n",
    "y_te = y_test[indices2]\n",
    "\n",
    "n1 = int(Ntr/batch_size)\n",
    "x_batches1 = np.array_split(x_tr,n1)\n",
    "y_batches1 = np.array_split(y_tr,n1)\n",
    "\n",
    "n2 = int(Nte/batch_size)\n",
    "x_batches2 = np.array_split(x_te,n2)\n",
    "y_batches2 = np.array_split(y_te,n2)\n",
    "\n",
    "\n",
    "for i in range(n1):\n",
    "    #train accuracy,train loss\n",
    "    h = 1.0/(1.0+np.exp(-(x_batches1[i].dot(w1)+b1)))\n",
    "    y_pred = h.dot(w2)+b2\n",
    "    train_acc = 1.0 -1/(9*batch_size)*(np.abs(np.argmax(y_batches1[i],axis=1) - np.argmax(y_pred, axis=1))).sum()\n",
    "    train_loss = 1/(batch_size)*np.square( y_batches1[i]- y_pred).sum()\n",
    "    #train_loss = 1.0 -1/(81*Ntr)*np.square(np.argmax(y_batches1[i],axis=1) - np.argmax(y_pred, axis=1)).sum() + reg*(np.sum(w2*w2)+np.sum(w1*w1))\n",
    "    #train_acc = 1/Ntr*(np.sum(np.argmax(y_pred,axis=1)==np.argmax(y_batches1[i],axis=1)))\n",
    "    #train_loss = 1./batch_size*np.square(y_pred-y_batches1[i]).sum()+reg*(np.sum(w2*w2)+np.sum(w1*w1))\n",
    "\n",
    "for i in range(n2):\n",
    "    #test accuracy, test loss\n",
    "    h = 1.0/(1.0+np.exp(-(x_batches2[i].dot(w1)+b1)))\n",
    "    y_pred = h.dot(w2)+b2\n",
    "    test_acc = 1.0 - 1/(9*batch_size)*(np.abs(np.argmax(y_batches2[i],axis=1) - np.argmax(y_pred, axis=1))).sum()\n",
    "    test_loss = 1/(batch_size)*np.square(y_batches2[i] - y_pred).sum()\n",
    "\n",
    "    #test_loss = 1.0 -1/(81*Nte)*np.square(np.argmax(y_batches2[i],axis=1) - np.argmax(y_pred, axis=1)).sum() + reg*(np.sum(w2*w2)+np.sum(w1*w1))\n",
    "    #test_acc = 1/Nte*(np.sum(np.argmax(y_pred,axis=1)==np.argmax(y_batches2[i],axis=1)))\n",
    "    #test_loss = 1./batch_size*np.square(y_pred-y_batches2[i]).sum()+reg*(np.sum(w2*w2)+np.sum(w1*w1))\n",
    "\n",
    "\n",
    "\n",
    "print(\"train_acc = \", train_acc)\n",
    "print(\"train_loss = \", train_loss)\n",
    "print(\"test_acc = \", test_acc)\n",
    "print(\"test_loss = \", test_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}